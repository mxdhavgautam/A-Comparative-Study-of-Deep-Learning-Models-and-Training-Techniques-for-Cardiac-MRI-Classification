# A Comparative Study of Deep Learning Models and Training Techniques for Cardiac MRI Classification
 This repository presents a comparative study of deep learning models (MobileNetV2, ResNet152V2, DenseNet201, InceptionV3) for classifying Normal vs. Sick Cardiac MRI images from a public Kaggle dataset (~63k images, ~1.5:1 ratio after processing). It includes implementations using transfer learning, data augmentation, class weighting, spatial attention, and ensemble averaging. The repository contains the Jupyter notebook for sequential training/evaluation, saved model checkpoints (.keras), training/evaluation plots, and the accompanying research paper, providing a comprehensive resource for automated CMRI analysis using CNNs.

## Project Overview

This repository contains the source code (as a consolidated Jupyter Notebook), trained models (excluding ResNet variants due to size), evaluation results, and research paper for a project focused on the automated classification of Cardiac Magnetic Resonance Imaging (CMRI) scans. The primary goal is to distinguish between 'Normal' and 'Sick' (abnormal) cardiac conditions using various deep learning techniques, comparing four prominent CNN architectures:
*   **MobileNetV2:** Known for computational efficiency.
*   **ResNet152V2:** A deep architecture utilizing residual connections.
*   **DenseNet201:** Characterized by dense connectivity for feature reuse.
*   **InceptionV3:** Employs multi-scale convolutional modules.

The study explores the effectiveness of transfer learning, data augmentation, class imbalance handling (using class weights), a simple spatial attention mechanism, and ensemble learning (averaging attention models).

## Dataset

The project utilizes the publicly available **CAD Cardiac MRI Dataset** sourced from Kaggle:
[https://www.kaggle.com/datasets/danialsharifrazi/cad-cardiac-mri-dataset](https://www.kaggle.com/datasets/danialsharifrazi/cad-cardiac-mri-dataset)

*   **Preprocessing:** The notebook includes steps to download the dataset, flatten the initial nested directory structure (moving images to top-level `Normal` and `Sick` folders), and then uses `ImageDataGenerator`'s `validation_split` (set to 0.2) to partition the data *internally* into training (80%) and validation (20%) sets. No separate test directory is created with this method.
*   **Verified Distribution:** After flattening, the processed dataset used for training/validation contained approximately 37.5k Normal and 25.8k Sick images (~1.5:1 ratio).
*   **Generators:** `ImageDataGenerator` is used for loading, rescaling ([0,1]), resizing (128x128), applying augmentation to the training subset, and splitting data.

## Methodology Implemented in Notebook

1.  **Setup & Data Prep:** Imports libraries, downloads data (requires `kaggle.json`), flattens directory structure.
2.  **Data Generators & Class Weights:** Creates `train_generator` and `val_generator` using `ImageDataGenerator(validation_split=0.2)`. Calculates balanced class weights based on the training subset.
3.  **Model Building:** Functions define baseline transfer learning models (MobileNetV2, ResNet152V2, DenseNet201, InceptionV3) and their attention-augmented counterparts. Base models are frozen.
4.  **Training:** Uses Adam optimizer (LR=1e-4), Binary Crossentropy loss. Employs class weighting, EarlyStopping, ReduceLROnPlateau, and ModelCheckpoint (saving best model based on `val_loss`). Training is set for 10 epochs.
5.  **Sequential Evaluation:** The notebook trains and evaluates each model configuration sequentially:
    *   Each baseline model evaluated on the **validation set**.
    *   Each attention-augmented model evaluated on the **validation set**.
    *   An averaging ensemble of the *attention-augmented* models evaluated on the **validation set**.
6.  **Results:** Performance is measured using Accuracy, Balanced Accuracy, Precision, Recall, F1-Score (per class), and Confusion Matrices. Training history is plotted.

## Repository Contents

*   `file.ipynb`: The main Jupyter Notebook containing all code.
*   `.keras files`: Saved best model checkpoints for **MobileNetV2, DenseNet201, and InceptionV3** (baseline and attention variants).
    *   **Note:** The ResNet152V2 baseline and attention model `.keras` files are **excluded** from this repository due to their large size (exceeding typical GitHub file size limits). They can be generated by running the corresponding cells within the Jupyter notebook.
*   `.png files` : Training history plots for all models.
*   `AComparativeStudyofDeepLearningModelsandTrainingTechniquesforCardiacMRIClassification.pdf`: The final research paper detailing the study.
*   `LICENSE`: Contains the MIT License text.
*   `README.md`: This file.

## How to Run

1.  **Environment:** Best run in Google Colab with GPU acceleration enabled. Ensure necessary libraries (TensorFlow, Scikit-learn, Pandas, Seaborn, Matplotlib) are installed.
2.  **Kaggle Credentials:** Upload your `kaggle.json` file when prompted by the notebook's download cell.
3.  **Execution:** Run the notebook cells sequentially from top to bottom.
    *   The initial cells handle setup, data download, and flattening.
    *   The data generator cell performs the 80/20 train/validation split internally.
    *   Subsequent cells build, train, and evaluate each model configuration (baselines, attention models, ensemble) **on the validation set**. Note: Training all models requires significant compute time.
4.  **Outputs:** Results (metrics, plots) are displayed inline. The best model checkpoints (`.keras`) and training history plots (`.png`) are saved to the specified directories within the Colab environment. The final cell can zip these for download. The ResNet models must be generated by running the notebook.

## Key Results Summary (Validation Set)

| Model                              | Accuracy | Balanced Acc. | Loss   | Precision (Normal) | Recall (Normal) | F1-Score (Normal) | Precision (Sick) | Recall (Sick) | F1-Score (Sick) |
| :--------------------------------- | :------- | :------------ | :----- | :----------------- | :-------------- | :---------------- | :--------------- | :------------ | :-------------- |
| MobileNetV2\_Baseline\_OG\_Split   | 0.7618   | 0.7815        | 0.5018 | 0.9016             | 0.6713          | 0.7700            | 0.6549           | 0.8917        | 0.7556          |
| ResNet152V2\_Baseline\_OG\_Split | 0.7744   | 0.7745        | 0.4853 | 0.8280             | 0.7749          | 0.8005            | 0.7027           | 0.7742        | 0.7368          |
| DenseNet201\_Baseline\_OG\_Split | 0.7631   | 0.7596        | 0.5063 | 0.8095             | 0.7814          | 0.7952            | 0.6996           | 0.7378        | 0.7182          |
| InceptionV3\_Baseline\_OG\_Split | 0.7015   | 0.6874        | 0.5470 | 0.7395             | 0.7588          | 0.7490            | 0.6432           | 0.6160        | 0.6293          |
| Mobilenetv2\_Attention\_OG\_Split  | 0.7267   | 0.7375        | 0.5115 | 0.8285             | 0.6761          | 0.7447            | 0.6315           | 0.7989        | 0.7052          |
| Resnet152v2\_Attention\_OG\_Split | 0.7409   | 0.7321        | 0.4923 | 0.7820             | 0.7801          | 0.7810            | 0.6817           | 0.6841        | 0.6829          |
| Densenet201\_Attention\_OG\_Split | 0.7439   | 0.7358        | 0.5020 | 0.7861             | 0.7796          | 0.7828            | 0.6837           | 0.6920        | 0.6878          |
| Inceptionv3\_Attention\_OG\_Split | 0.7036   | 0.6946        | 0.5530 | 0.7530             | 0.7433          | 0.7481            | 0.6340           | 0.6458        | 0.6398          |
| **Attention\_Ensemble\_OG\_Split** | **0.7780** | **0.7722**    | **N/A**| **0.8184**         | **0.8034**      | **0.8108**        | **0.7218**       | **0.7411**    | **0.7313**      |

*(Metrics reported on the 20% validation split of the ~1.5:1 ratio dataset)*

## Conclusion

This study successfully applied deep transfer learning for classifying cardiac MRI images based on a moderately imbalanced dataset (~1.5:1 ratio). MobileNetV2 and ResNet152V2 baselines showed the strongest individual performance (Balanced Accuracy ~77-78%), highlighting the accuracy-efficiency trade-off. Class weighting was essential. The simple attention mechanism did not yield improvements, while the attention model ensemble provided robust, competitive performance.
